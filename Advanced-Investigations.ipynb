{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "dBvv3tArwW1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hurst\n",
        "!pip install pyinform\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from statsmodels.tsa.stattools import adfuller, kpss\n",
        "from hurst import compute_Hc\n",
        "import pyinform.transferentropy as te\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_iris\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')  # This stops it showing warnings about \"smaller than value shown.\""
      ],
      "metadata": {
        "id": "w7yL09m57sj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate Data - note added noise for the trend up and down lines as many things don't work with perfectly aligned data.\n",
        "\n",
        "Data used for everything up to Fourier Transform"
      ],
      "metadata": {
        "id": "8wYjBxh48HfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Stationary - all same value\n",
        "stationary = np.array([4]+[5] * 98 + [6])\n",
        "\n",
        "# 2. Trending up - simple +1 increments\n",
        "trending_up = np.arange(100) + np.random.normal(0, 0.5, 100)\n",
        "\n",
        "# 3. Trending down - simple -1 decrements\n",
        "trending_down = trending_up[::-1]  # Flip it to a down trend.\n",
        "\n",
        "# 4. Cycling - simple sine wave pattern\n",
        "cycling = np.array([0, 1, 2, 1, 0, -1, -2, -1] * 13)[:100]  # Repeat pattern\n",
        "\n",
        "# 5. Exponential up - exponential growth\n",
        "exponential_up = np.array([2**i for i in range(100)]) + np.random.normal(0, 1e5, 100)"
      ],
      "metadata": {
        "id": "QVEAWT018H7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 5, figsize=(18, 4))  # 1 row, 5 columns\n",
        "\n",
        "axes[0].plot(stationary)\n",
        "axes[0].set_title(\"Stationary\")\n",
        "\n",
        "axes[1].plot(trending_up)\n",
        "axes[1].set_title(\"Trending Up\")\n",
        "\n",
        "axes[2].plot(trending_down)\n",
        "axes[2].set_title(\"Trending Down\")\n",
        "\n",
        "axes[3].plot(cycling)\n",
        "axes[3].set_title(\"Cycling\")\n",
        "\n",
        "axes[4].plot(exponential_up)\n",
        "axes[4].set_title(\"Exponential Up\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DCZey5n8Fi6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Augmented Dickey-Fuller Test"
      ],
      "metadata": {
        "id": "9WO5enQc7v5p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result1 = adfuller(stationary)\n",
        "result2 = adfuller(trending_up)\n",
        "result3 = adfuller(trending_down)\n",
        "result4 = adfuller(cycling)\n",
        "result5 = adfuller(exponential_up)\n",
        "print(f\"Stationary P-Value: {result1[1]}\")  # P-value < 0.05 then reject default hypothesis = stationary\n",
        "print(f\"Trending Up P-Value: {result2[1]}\")  # P-value > 0.05 then accept default hypothesis = non-stationary\n",
        "print(f\"Trending Down P-Value: {result3[1]}\")  # P-value > 0.05 then accept default hypothesis = non-stationary\n",
        "print(f\"Cycling P-Value: {result4[1]}\")  # P-value < 0.05 then reject default hypothesis = stationary (cycling around a flat line)\n",
        "print(f\"Exponential Up P-Value: {result5[1]}\")  # P-value > 0.05 then accept default hypothesis = non-stationary"
      ],
      "metadata": {
        "id": "4r9D5EDp7yMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kwiatkowski-Phillips-Schmidt-Shin Test"
      ],
      "metadata": {
        "id": "a6UUHlr0_u4y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Regression='c' is like ADF test. Regression='ct' is trend stationary.\n",
        "result1 = kpss(stationary, regression='ct')\n",
        "result2 = kpss(trending_up, regression='ct')\n",
        "result3 = kpss(trending_down, regression='ct')\n",
        "result4 = kpss(cycling, regression='ct')\n",
        "result5 = kpss(exponential_up, regression='ct')\n",
        "print(f\"Stationary P-Value: {result1[1]}\")  # P-value > 0.05 then accept default hypothesis = trend-stationary (cycling around a flat line)\n",
        "print(f\"Trending Up P-Value: {result2[1]}\")  # P-value > 0.05 then accept default hypothesis = trend-stationary (cycling around a flat line)\n",
        "print(f\"Trending Down P-Value: {result3[1]}\")  # P-value > 0.05 then accept default hypothesis = trend-stationary (cycling around a flat line)\n",
        "print(f\"Cycling P-Value: {result4[1]}\")  # P-value > 0.05 then accept default hypothesis = trend-stationary (cycling around a flat line)\n",
        "print(f\"Exponential Up P-Value: {result5[1]}\")  # P-value < 0.05 then reject default hypothesis = non-trend stationary"
      ],
      "metadata": {
        "id": "imXWRbIW_wBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hurst Exponent"
      ],
      "metadata": {
        "id": "D3Fyz_sMGAvv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result1 = compute_Hc(stationary)\n",
        "result2 = compute_Hc(trending_up)\n",
        "result3 = compute_Hc(trending_down)\n",
        "result4 = compute_Hc(cycling)\n",
        "result5 = compute_Hc(exponential_up)\n",
        "\n",
        "print(f\"Stationary P-Value: {result1[0]}\")  # H > 0.5 = linear trend [but is close to 0.5, so closer to a random walk]\n",
        "print(f\"Trending Up P-Value: {result2[0]}\")  # H > 0.5 = linear trend\n",
        "print(f\"Trending Down P-Value: {result3[0]}\")  # H > 0.5 = linear trend\n",
        "print(f\"Cycling P-Value: {result4[0]}\")  # H < 0.5 = mean reversion (cycling)\n",
        "print(f\"Exponential Up P-Value: {result5[0]}\")  # H > 0.5 = linear trend [but is close to 0.5, so closer to a random walk]\n",
        "\n",
        "# Horizontal trends or exponential trends will appear as a random walk because the function isn't really equipped to handle it.\n",
        "\n"
      ],
      "metadata": {
        "id": "rDR-1SEBGB2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fourier Analysis / Transform"
      ],
      "metadata": {
        "id": "BL-wgFBI9X4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create time scale\n",
        "time = np.arange(0, 10, 0.01)  # start 0, end 10, in steps of 0.1\n",
        "\n",
        "# Create 3 waves with amplitudes 1, 0.5, 0.3\n",
        "wave1 = 1.0 * np.sin(2 * np.pi * 1 * time)   # 1 Hz\n",
        "wave2 = 0.5 * np.sin(2 * np.pi * 3 * time)   # 3 Hz\n",
        "wave3 = 0.3 * np.sin(2 * np.pi * 5 * time)   # 5 Hz\n",
        "\n",
        "# Combine waves\n",
        "complex_wave = wave1 + wave2 + wave3\n",
        "\n",
        "# Plot all waves in one figure with 4 subplots\n",
        "fig, axs = plt.subplots(4, 1, figsize=(12, 12), sharex=True)\n",
        "\n",
        "axs[0].plot(time, wave1, color='blue')\n",
        "axs[0].set_ylabel('Amplitude')\n",
        "axs[0].set_title('Wave 1 (1 Hz)')\n",
        "\n",
        "axs[1].plot(time, wave2, color='orange')\n",
        "axs[1].set_ylabel('Amplitude')\n",
        "axs[1].set_title('Wave 2 (3 Hz)')\n",
        "\n",
        "axs[2].plot(time, wave3, color='green')\n",
        "axs[2].set_ylabel('Amplitude')\n",
        "axs[2].set_title('Wave 3 (5 Hz)')\n",
        "\n",
        "axs[3].plot(time, complex_wave, color='red')\n",
        "axs[3].set_xlabel('Time')\n",
        "axs[3].set_ylabel('Amplitude')\n",
        "axs[3].set_title('Combined Complex Wave')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Perform FFT\n",
        "fft_result = np.fft.fft(complex_wave)\n",
        "\n",
        "# Frequency vector - get the frequencies\n",
        "n = len(complex_wave)  # n is number of values\n",
        "freq = np.fft.fftfreq(n, d=0.01)  # d is sampling frequency, steps from the original time series being created.\n",
        "\n",
        "# Amplitude spectrum - get how strong the frequencies are\n",
        "amp = np.abs(fft_result) / n\n",
        "\n",
        "# Plot FFT spectrum (full range)\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(freq, amp, color='purple')\n",
        "plt.xlabel('Frequency (Hz)')\n",
        "plt.ylabel('Amplitude')\n",
        "plt.title('Frequency Spectrum')\n",
        "plt.show()\n",
        "\n",
        "# Plot FFT spectrum limited to 0-10 Hz\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(freq, amp, color='purple')\n",
        "plt.xlim(0, 10)\n",
        "plt.xlabel('Frequency (Hz)')\n",
        "plt.ylabel('Amplitude')\n",
        "plt.title('Frequency Spectrum (0-10 Hz)')\n",
        "plt.show()\n",
        "\n",
        "# Inverse FFT to reconstruct time series\n",
        "inv_fft = np.fft.ifft(fft_result)\n",
        "recon = np.real(inv_fft)  # Take the real part\n",
        "\n",
        "# Plot reconstructed time series\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(time, recon, color='brown')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Amplitude')\n",
        "plt.title('Reconstructed Time Series')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1nAC3yCT9YiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transfer Entropy"
      ],
      "metadata": {
        "id": "2BbssS0m994m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Step 1: Generate the data\n",
        "# ----------------------------\n",
        "\n",
        "#  X is values 0-501, times 0.2 and add noise.\n",
        "#  Y is values of X plus noise.\n",
        "\n",
        "np.random.seed(0)\n",
        "n = 500\n",
        "x = np.zeros(n + 1)\n",
        "y = np.zeros(n + 1)\n",
        "\n",
        "for i in range(n):\n",
        "    x[i + 1] = 0.2 * x[i] + np.random.normal(0, 2)\n",
        "    y[i + 1] = x[i] + np.random.normal(0, 2)\n",
        "\n",
        "x = x[1:]\n",
        "y = y[1:]\n",
        "\n",
        "# Plotting\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "axes[0].plot(x, color=\"blue\")\n",
        "axes[0].set_title(\"X\")\n",
        "axes[1].plot(y, color=\"green\")\n",
        "axes[1].set_title(\"Y\")\n",
        "axes[2].scatter(x, y, alpha=0.5, color=\"red\")\n",
        "axes[2].set_title(\"X vs Y\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ----------------------------\n",
        "# Step 2: Bin the continuous data - put data into categories (bins)\n",
        "# ----------------------------\n",
        "def bin_series(series, bins=10):\n",
        "    return np.digitize(series, bins=np.linspace(np.min(series), np.max(series), bins))\n",
        "\n",
        "x_binned = bin_series(x, bins=10)\n",
        "y_binned = bin_series(y, bins=10)\n",
        "\n",
        "# ----------------------------\n",
        "# Step 2: Define bootstrap/surrogate function\n",
        "# Bootstrapping is shuffle the values to remove the time element and recalculate TE multiple times to confirm significance.\n",
        "# ----------------------------\n",
        "def bootstrap_te_significance(source, target, k=1, nboot=1000, seed=0):\n",
        "    np.random.seed(seed)\n",
        "    obs_te = te.transfer_entropy(source, target, k=k)\n",
        "\n",
        "    surrogate_te = []\n",
        "    for _ in range(nboot):\n",
        "        source_shuffled = np.random.permutation(source)\n",
        "        te_val = te.transfer_entropy(source_shuffled, target, k=k)\n",
        "        surrogate_te.append(te_val)\n",
        "\n",
        "    surrogate_te = np.array(surrogate_te)\n",
        "    # p-value = fraction of surrogate TE >= observed TE\n",
        "    p_value = np.mean(surrogate_te >= obs_te)\n",
        "\n",
        "    return obs_te, p_value, surrogate_te\n",
        "\n",
        "# ----------------------------\n",
        "# Step 3: Run for X → Y\n",
        "# ----------------------------\n",
        "obs_te_xy, p_xy, surrogate_xy = bootstrap_te_significance(x_binned, y_binned, k=1, nboot=1000)\n",
        "obs_te_yx, p_yx, surrogate_yx = bootstrap_te_significance(y_binned, x_binned, k=1, nboot=1000)\n",
        "\n",
        "# ANSI escape codes for bold on/off\n",
        "BOLD = \"\\033[1m\"\n",
        "RESET = \"\\033[0m\"\n",
        "\n",
        "def format_pval(p):\n",
        "    return f\"{BOLD}{p:.4f}{RESET}\" if p < 0.01 else f\"{p:.4f}\"\n",
        "\n",
        "print()\n",
        "print(f\"X → Y: TE = {obs_te_xy:.4f}, p-value = {format_pval(p_xy)}\")\n",
        "print(f\"Y → X: TE = {obs_te_yx:.4f}, p-value = {format_pval(p_yx)}\")\n",
        "\n",
        "# p-values: < 0.001 '***', < 0.01 '**', < 0.05 '*', < 0.1 '.'   Smaller the P value, the more significant the transfer. 0.1 or over is \"meh\""
      ],
      "metadata": {
        "id": "PXTWitKS9_lt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the iris dataset\n",
        "iris_data = load_iris()\n",
        "X = iris_data.data       # Features (columns 1-4 in R)\n",
        "y = iris_data.target     # Species labels as numbers\n",
        "species_names = iris_data.target_names[y]  # Convert to species names\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Perform PCA\n",
        "pca = PCA()\n",
        "pca_scores = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Create a DataFrame with PCA scores and species\n",
        "scores_df = pd.DataFrame(pca_scores, columns=[f'PC{i+1}' for i in range(X.shape[1])])\n",
        "scores_df['Species'] = species_names\n",
        "\n",
        "# Plot the first two principal components\n",
        "plt.figure(figsize=(8,6))\n",
        "for species in np.unique(species_names):\n",
        "    plt.scatter(scores_df.loc[scores_df['Species'] == species, 'PC1'],\n",
        "                scores_df.loc[scores_df['Species'] == species, 'PC2'],\n",
        "                label=species)\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "plt.title('PCA of Iris Dataset')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Print PCA components (loadings)\n",
        "print(\"PCA Components (Loadings):\")\n",
        "print(pd.DataFrame(pca.components_, columns=iris_data.feature_names, index=[f'PC{i+1}' for i in range(X.shape[1])]))\n",
        "print()\n",
        "\n",
        "# Variance explained by each principal component\n",
        "var_explained = pca.explained_variance_ratio_\n",
        "for i, var in enumerate(var_explained):\n",
        "    print(f\"PC{i+1}: {var*100:.2f}%\")\n",
        "\n",
        "# PC1 scores\n",
        "pc1_scores = pca_scores[:, 0]\n",
        "\n",
        "# Plot PC1 for each species\n",
        "plt.figure(figsize=(8, 4))\n",
        "for species in np.unique(species_names):\n",
        "    plt.scatter(pc1_scores[species_names == species],\n",
        "                [0]*sum(species_names == species),  # all points on y=0 for a 1D view\n",
        "                label=species, s=50)  # s=50 makes the points bigger\n",
        "\n",
        "plt.xlabel('PC1 (Flower Size Dimension)')\n",
        "plt.yticks([])  # hide y-axis since it's not informative\n",
        "plt.title('Separation of Iris Species Along PC1')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bJmQQnUmRLkO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}